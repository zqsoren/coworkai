"""
Graph Nodes - LangGraph å„èŠ‚ç‚¹å®ç°
Router â†’ Agent â†’ Tool â†’ Approval â†’ End
"""

import json
import re
from typing import Optional

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from .state import AgentState


def _get_llm(agent_config: dict):
    """æ ¹æ® Agent é…ç½®è·å–å¯¹åº”çš„ LLM å®ä¾‹"""
    from src.core.llm_manager import LLMManager
    
    mgr = LLMManager()
    
    # 1. ä¼˜å…ˆä½¿ç”¨ provider_id + model_name
    provider_id = agent_config.get("provider_id")
    model_name = agent_config.get("model_name")
    
    # è·å–å¯¹åº”çš„ Provider
    if provider_id:
        provider = mgr.get_provider(provider_id)
        if provider and (not model_name or str(model_name).strip() == ""):
            # å¦‚æœ Agent æ²¡æœ‰æ˜¾å¼æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨ Provider ä¸­é…ç½®çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
            if provider.models and len(provider.models) > 0:
                model_name = provider.models[0]
    
    # 2. å…¼å®¹æ—§ç‰ˆ model_tier
    if not provider_id or not model_name:
        tier = agent_config.get("model_tier", "tier1")
        # ç®€å•æ˜ å°„ fallback
        # ç†æƒ³æƒ…å†µä¸‹åº”è¯¥è¯»å–æ—§çš„ secrets["models"]ï¼Œä½†ä¸ºäº†æ¶æ„æ•´æ´ï¼Œæˆ‘ä»¬ç›´æ¥ default åˆ° gemini/openai
        if tier == "tier1":
            provider_id = "gemini_default" 
            model_name = "gemini-1.5-pro"
        else:
            provider_id = "gemini_default"
            model_name = "gemini-2.0-flash"
            
    try:
        return mgr.get_model(provider_id, model_name)
    except Exception as e:
        # Fallback if specific provider/model fails or doesn't exist
        print(f"Error initializing LLM ({provider_id}/{model_name}): {e}")
        # Try a safe default
        try:
            return mgr.get_model("gemini_default", "gemini-2.0-flash")
        except:
            raise ValueError(f"æ— æ³•åˆå§‹åŒ– LLMï¼Œè¯·æ£€æŸ¥è®¾ç½®: {e}")


def _get_tools(agent_config: dict, base_path: str = None) -> list:
    """æ ¹æ® Agent é…ç½®è·å–å·¥å…·å’ŒæŠ€èƒ½åˆ—è¡¨
    
    Args:
        agent_config: Agent é…ç½®å­—å…¸
        base_path: Agent æ ¹ç›®å½• (ç”¨äºä¸Šä¸‹æ–‡æ„ŸçŸ¥å·¥å…·)ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€å·¥å…·
    """
    from langchain_core.tools import StructuredTool
    from src.tools.file_tools import FILE_TOOLS, create_agent_file_tools, _file_manager
    from src.tools.web_tools import WEB_TOOLS
    from src.tools.code_tools import CODE_TOOLS
    from src.tools.browser_tools import BROWSER_TOOLS
    from src.tools.meta_tools import META_TOOLS
    from src.skills.skill_loader import SkillLoader
    import os

    # 1. æ”¶é›† L1 Tools
    # å¦‚æœæä¾›äº† base_path ä¸” _file_manager å·²åˆå§‹åŒ–ï¼Œåˆ™ä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ File Tools
    if base_path and _file_manager:
        file_tools = create_agent_file_tools(base_path, _file_manager)
    else:
        file_tools = FILE_TOOLS

    all_tools = {t.name: t for t in file_tools + WEB_TOOLS + CODE_TOOLS + BROWSER_TOOLS + META_TOOLS}

    # 2. æ”¶é›† L2/L3 Skills
    # è¿™é‡Œå‡è®¾ custom_skills åœ¨é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sl = SkillLoader(os.path.join(project_root, "custom_skills"))
    sl.scan_and_load()

    for name, skill_data in sl.skills.items():
        # å°†æŠ€èƒ½å‡½æ•°åŒ…è£…ä¸º LangChain Tool
        # æ³¨æ„: éœ€è¦æ•è· closure å˜é‡
        def create_wrapper(run_func):
            def wrapper(**kwargs):
                return run_func(**kwargs)
            return wrapper
        
        wrapper_func = create_wrapper(skill_data["run"])
        
        tool = StructuredTool.from_function(
            func=wrapper_func,
            name=skill_data["name"],
            description=skill_data["description"]
        )
        all_tools[skill_data["name"]] = tool

    # 3. è¿‡æ»¤
    requested_tools = agent_config.get("tools", [])
    requested_skills = agent_config.get("skills", [])
    
    final_tools = []
    # åˆå¹¶ tools å’Œ skills åˆ—è¡¨
    for name in requested_tools + requested_skills:
        if name in all_tools:
            final_tools.append(all_tools[name])
            
    return final_tools


def router_node(state: AgentState) -> dict:
    """
    è·¯ç”±èŠ‚ç‚¹ï¼šè§£æç”¨æˆ·è¾“å…¥ï¼Œæ£€æµ‹ @mentionï¼Œå†³å®šè·¯ç”±ç›®æ ‡ã€‚
    """
    messages = state.get("messages", [])
    if not messages:
        return state

    last_msg = messages[-1]
    content = last_msg.content if hasattr(last_msg, "content") else str(last_msg)

    # æ£€æµ‹ @mention
    mention_match = re.search(r"@(\w+)", content)
    if mention_match:
        target = mention_match.group(1)
        # ä¸Šä¸‹æ–‡åˆ‡ç‰‡ï¼šæå–æœ€è¿‘ 3 æ¡æ¶ˆæ¯ä½œä¸ºæ‘˜è¦
        recent = messages[-4:-1] if len(messages) > 3 else messages[:-1]
        summary_parts = []
        for msg in recent:
            role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "åŠ©æ‰‹"
            msg_content = msg.content if hasattr(msg, "content") else str(msg)
            summary_parts.append(f"{role}: {msg_content[:200]}")
        mention_summary = "\n".join(summary_parts) if summary_parts else "æ— å‰æ–‡ä¸Šä¸‹æ–‡"

        return {
            "mention_target": target,
            "mention_summary": mention_summary,
        }

    return {
        "mention_target": None,
        "mention_summary": None,
    }


def agent_node(state: AgentState) -> dict:
    """
    Agent èŠ‚ç‚¹ï¼šè°ƒç”¨ LLMï¼Œç»‘å®šå·¥å…·ã€‚
    """
    agent_config = state.get("agent_config", {})
    context = state.get("context", "")
    messages = state.get("messages", [])

    try:
        llm = _get_llm(agent_config)
    except ValueError as e:
        return {
            "messages": [AIMessage(content=f"âš ï¸ é…ç½®é”™è¯¯: {str(e)}")],
            "needs_approval": False,
        }

    # æ„å»º system prompt
    system_prompt = agent_config.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ã€‚")
    if context:
        system_prompt += f"\n\n---\n{context}"


    # Agentic RAG: æç¤ºè¯å¢å¼º
    system_prompt += """

ä½ æ˜¯ä¸€ä¸ªé«˜çº§ AI åŠ©æ‰‹ã€‚ä½ å¯ä»¥ä½¿ç”¨ `search_knowledge_base` å·¥å…·ã€‚
**é‡è¦æç¤º**ï¼šä½ é»˜è®¤ä¸çŸ¥é“ç”¨æˆ·æ•°æ®åº“ä¸­çš„å†…å®¹ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ç‰¹å®šçš„ IDã€æŸä»½æ–‡æ¡£æˆ–é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†ï¼Œä½ å¿…é¡»é¦–å…ˆè°ƒç”¨ `search_knowledge_base` å·¥å…·æ¥æ”¶é›†ä¿¡æ¯ã€‚
ç»å¯¹ä¸è¦ççŒœã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œç”Ÿæˆç²¾å‡†çš„æœç´¢æŸ¥è¯¢è¯ï¼Œè°ƒç”¨è¯¥å·¥å…·ï¼Œç„¶åä½¿ç”¨è¿”å›çš„çœŸå®ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·ã€‚
"""

    # æ·»åŠ  @mention ä¸Šä¸‹æ–‡
    mention_summary = state.get("mention_summary")
    if mention_summary:
        system_prompt += f"\n\n---\n## å‰æ–‡ä¸Šä¸‹æ–‡ï¼ˆç”±ä¸»å¯¹è¯ä¼ é€’ï¼‰\n{mention_summary}"

    # è·å–å·¥å…·
    import os
    base_path = None
    rag_tool = None
    
    curr_ws = state.get("current_workspace")
    curr_agent = state.get("current_agent")
    
    if curr_ws and curr_agent:
        base_path = os.path.join(curr_ws, curr_agent)
        
        # ğŸ†• Agentic RAG: Bind Search Tool
        try:
            from src.utils.rag_ingestion import RAGIngestion
            from src.tools.rag_tools import get_rag_tool
            
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            data_root = os.path.join(project_root, "data")
            
            # Check if agent dir exists to avoid errors
            if os.path.exists(os.path.join(data_root, curr_ws, curr_agent)):
                 rag = RAGIngestion(data_root, curr_ws, curr_agent)
                 rag_tool = get_rag_tool(rag)
        except Exception as e:
            print(f"[nodes.py] RAG Tool Init Failed: {e}")

    tools = _get_tools(agent_config, base_path)
    
    if rag_tool:
        tools.append(rag_tool)

    # ç»‘å®šå·¥å…·åˆ° LLM
    if tools:
        llm_with_tools = llm.bind_tools(tools)
    else:
        llm_with_tools = llm

    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    chat_messages = [SystemMessage(content=system_prompt)] + list(messages)

    # è°ƒç”¨ LLM
    try:
        response = llm_with_tools.invoke(chat_messages)
    except Exception as e:
        return {
            "messages": [AIMessage(content=f"âš ï¸ LLM è°ƒç”¨å¤±è´¥: {str(e)}")],
            "needs_approval": False,
        }

    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    has_tool_calls = hasattr(response, "tool_calls") and response.tool_calls
    
    return {
        "messages": [response],
        "needs_approval": False,
    }


def tool_node(state: AgentState) -> dict:
    """
    å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ï¼šæ‰§è¡Œ LLM è¿”å›çš„å·¥å…·è°ƒç”¨ã€‚
    """
    from langchain_core.messages import ToolMessage

    messages = state.get("messages", [])
    agent_config = state.get("agent_config", {})
    last_msg = messages[-1] if messages else None

    if not last_msg or not hasattr(last_msg, "tool_calls") or not last_msg.tool_calls:
        return {"messages": [], "pending_changes": [], "needs_approval": False}

    tools = _get_tools(agent_config) # è¿™é‡Œçš„ tools å¯èƒ½æ²¡æœ‰ context?
    # Tool Node ä¹Ÿè¦é‡æ–°è·å– context aware tools å—ï¼Ÿ
    # æ˜¯çš„ï¼Œå› ä¸º StructuredTool é—­åŒ…äº† contextã€‚
    base_path = None
    curr_ws = state.get("current_workspace")
    curr_agent = state.get("current_agent")
    if curr_ws and curr_agent:
        import os
        base_path = os.path.join(curr_ws, curr_agent)

    tools = _get_tools(agent_config, base_path)
    tool_map = {t.name: t for t in tools}

    new_messages = []
    pending_changes = list(state.get("pending_changes", []))
    needs_approval = False

    for call in last_msg.tool_calls:
        tool_name = call["name"]
        tool_args = call["args"]

        if tool_name in tool_map:
            try:
                result = tool_map[tool_name].invoke(tool_args)

                # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å« ChangeRequest
                if isinstance(result, str) and '"type": "change_request"' in result:
                    try:
                        cr_data = json.loads(result)
                        if cr_data.get("type") == "change_request":
                            pending_changes.append(cr_data)
                            needs_approval = True
                            result = f"ğŸ“‹ å·²ç”Ÿæˆæ–‡ä»¶å˜æ›´è¯·æ±‚: {cr_data.get('file_path', 'æœªçŸ¥')}\nè¯·åœ¨å³ä¾§å®¡æ‰¹é¢æ¿ä¸­æŸ¥çœ‹å·®å¼‚å¹¶å†³å®šæ˜¯å¦åº”ç”¨ã€‚"
                    except json.JSONDecodeError:
                        pass

                new_messages.append(
                    ToolMessage(content=str(result), tool_call_id=call["id"])
                )

                # Flight Recorder: è®°å½•å·¥å…·è°ƒç”¨
                _log_tool_call(state, tool_name, tool_args, "Success")

            except Exception as e:
                new_messages.append(
                    ToolMessage(content=f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}", tool_call_id=call["id"])
                )
                _log_tool_call(state, tool_name, tool_args, f"Error: {e}")
        else:
            new_messages.append(
                ToolMessage(content=f"å·¥å…· '{tool_name}' ä¸å¯ç”¨ã€‚", tool_call_id=call["id"])
            )

    return {
        "messages": new_messages,
        "pending_changes": pending_changes,
        "needs_approval": needs_approval,
    }


def approval_node(state: AgentState) -> dict:
    """
    å®¡æ‰¹èŠ‚ç‚¹ï¼šç­‰å¾…ç”¨æˆ·å¯¹æ–‡ä»¶å˜æ›´çš„å®¡æ‰¹ã€‚
    å®é™…å®¡æ‰¹é€šè¿‡ Streamlit UI çš„å›è°ƒå¤„ç†ã€‚
    æ­¤èŠ‚ç‚¹çš„ä½œç”¨æ˜¯æ ‡è®°æµç¨‹è¿›å…¥å®¡æ‰¹ç­‰å¾…çŠ¶æ€ã€‚
    """
    return {
        "approval_status": "waiting",
    }


# ----- æ¡ä»¶è¾¹ (Conditional Edges) -----

def should_use_tools(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    messages = state.get("messages", [])
    if messages:
        last_msg = messages[-1]
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            return "tools"
    return "respond"


def should_approve(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦å®¡æ‰¹"""
    if state.get("needs_approval", False):
        return "approval"
    
    # å·¥å…·æ‰§è¡Œåï¼Œè¿˜éœ€è¦è®© LLM ç”Ÿæˆæœ€ç»ˆå›å¤
    messages = state.get("messages", [])
    if messages:
        from langchain_core.messages import ToolMessage
        if isinstance(messages[-1], ToolMessage):
            return "continue"
    
    return "respond"


def after_approval(state: AgentState) -> str:
    """å®¡æ‰¹åçš„åˆ†æ”¯"""
    status = state.get("approval_status", "")
    if status == "approved":
        return "continue"
    elif status == "rejected":
        return "respond"
    return "wait"


# ----- Flight Recorder Helper -----

def _log_tool_call(state: dict, tool_name: str, args: dict, status: str):
    """è®°å½•å·¥å…·è°ƒç”¨åˆ° ProjectLogger (fail-safe)"""
    try:
        from src.core.project_logger import ProjectLogger
        import os
        ws = state.get("current_workspace")
        agent = state.get("current_agent")
        if ws and agent:
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            logger = ProjectLogger(os.path.join(project_root, "data"), ws, agent)
            logger.log_tool_call(tool_name, args, status)
    except Exception:
        pass  # æ—¥å¿—å¤±è´¥ä¸ä¸­æ–­ä¸»æµç¨‹
